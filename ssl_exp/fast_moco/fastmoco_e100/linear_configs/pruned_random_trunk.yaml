model:                              # architecture details
    type: resnet50_official         # model name
    kwargs:
        freeze_layer: True
        num_classes: 1000            # dimension of features
        bn:
            use_sync_bn: False      # whether to use syncbn
            kwargs: {}


optimizer:                  # optimizer details
    type: LARS
    kwargs:
        nesterov: False
        momentum: 0.9
        weight_decay: 0.0
        implementation: PyTorch

lr_scheduler:                   # learning rate scheduler details
    type: CosineEpoch
    kwargs:
        base_lr: 0.8 # 0.32 #       # initial leaning rate
        warmup_lr: 0.8 # 0.32 #        # learning rate after warming up
        warmup_epoch: 0 #25000         # iterations of warmup
        min_lr: 0.0             # mimimal learning rate
        max_epoch: 90 #28150       # total iterations of training 1281167

lms:                      # large model support: utilize cpu to save gpu memory
    enable: False         # whether to use lms
    kwargs:
        limit: 12         # the soft limit in G-bytes on GPU memory allocated for tensors
root_path: '/nfs/projects/healthcare/nasir/Fast-MoCo/ssl_exp/fast_moco/fastmoco_e100/experiments_linear'
data:                     # data details
    type: imagenet        # choices = {'imagenet', 'custom'}
    read_from: fs         # choices = {'mc', 'fs', 'fake', 'osg'}
    
    batch_size: 512         # batch size in one GPU
    num_workers: 10        # number of subprocesses for data loading
    pin_memory: True      # whether to copy Tensors into CUDA pinned memory
    input_size: 224       # training image size         # DIISABLED IN TRANSFORM
    test_resize: 256      # testing resize image size   # DIISABLED IN TRANSFORM

    train:
        root_dir:  /nfs/users/ext_prateek.munjal/data/imagenet_2012/train/  # Imagenet dataset
        meta_file:  /nfs/projects/healthcare/nasir/imagenet1k/train.txt   # Imagenet dataset meta file
        # meta_file:  /nfs/projects/healthcare/nasir/imagenet1k/train.txt   # Imagenet dataset meta file
        # meta_file:  /nfs/projects/healthcare/nasir/imagenet1k/prune_strategies/forget_events/drop_atleast_2_71p.txt   # Imagenet dataset meta file
        # meta_file:  /nfs/projects/healthcare/nasir/imagenet1k/prune_strategies/sum_preds/train_7.txt   # Imagenet dataset meta file
        image_reader:
            type: pil
        sampler:
            type: distributed_epoch #distributed_iteration
        transforms:
            type: LINEAR

    test:
        root_dir:  /nfs/users/ext_prateek.munjal/data/imagenet_2012/val/   # Imagenet val dataset
        # root_dir:  ..path/..to/images/mages/val/   # Imagenet val dataset
        meta_file:  /nfs/projects/healthcare/nasir/imagenet1k/val.txt   # Imagenet val dataset meta file
        image_reader:
            type: pil
        sampler:
            type: distributed
        transforms:
            type: ONECROP

        evaluator:
            type: imagenet
            kwargs:
                topk: [ 1, 5 ]

saver:                                # saving or loading details
    print_freq: 10                    # frequence of printing logger
    val_freq: 300                    # frequence of evaluating during training
    save_many: False                   # whether to save checkpoints after every evaluation
    pretrain:  
        path: '/nfs/projects/healthcare/nasir/Fast-MoCo/ssl_exp/fast_moco/fastmoco_e100/experiments_ssl/100/rand_forget_2_71p/checkpoints/ckpt.pth'

        # path: '/nfs/projects/healthcare/nasir/Fast-MoCo/ssl_exp/fast_moco/fastmoco_e100/checkpoints/ckpt.pth'
                    # pretrain model details
#        path: ***

    #     ignore:                     # ignore keys in checkpoints
    #         key:                    # if training from scratch, pop 'optimzier' and 'last_iter'
    #             - optimizer         # if resuming from ckpt, DO NOT pop them
    #             - last_iter
    #         model:                  # ignore modules in model
    #             - module.fc.weight  # if training with different number of classes, pop the keys
    #             - module.fc.bias    # of last fully-connected layers
